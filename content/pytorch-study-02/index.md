---
emoji: ğŸ”¥
title: Review AI504 Practice Session - 02 Basic ML
date: '2022-03-31 01:00:00'
author: ì„ ê²½
tags: Matplotlib LinearRegression Classification DeepLearning
categories: Deeplearning 
---

## 1. Matplotlib

NumPy ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í™œìš©í•˜ì—¬ ê·¸ë˜í”„ë¥¼ ë§Œë“¤ì–´ ì‹œê°í™”í•  ìˆ˜ ìˆëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ì´ë‹¤.

ì§€ë‚œë²ˆ NumPyì— ëŒ€í•œ ê¸°ë³¸ì ì¸ ì„¤ëª…ì€ ì—¬ê¸°ì—! ğŸ‘‡ğŸ¼

[https://sunkyoung.github.io/pytorch-study-01/](https://sunkyoung.github.io/pytorch-study-01/)

- How to use ?

```python
import matplotlib.pyplot as plt
import numpy as np
```

- Basic usage
    - plt.plot(x-axis, y-axis) : ì£¼ì–´ì§„ xì¶•, yì¶• ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì„ í˜• ê·¸ë˜í”„ë¥¼ ê·¸ë¦¼
    - plt.scatter(x-axis, y-axis, s=None, c=None) : ì£¼ì–´ì§„ xì¶•, yì¶• ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì ì„ ê·¸ë¦¼
        - s : ì ì˜ í¬ê¸°
        - c : ìƒ‰ê¹” ì§€ì • (ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œë„ ì§€ì •ê°€ëŠ¥í•˜ë©° ë¦¬ìŠ¤íŠ¸ ê¸¸ì´ë§Œí¼ cmapê³¼ normì„ mappingí•˜ì—¬ ìƒ‰ì„ í‘œí˜„)
        - e.g. `plt.scatter(X, y, s=30, c="red")`
    - plt.show() : í”Œë¡¯ì„ ë³´ì—¬ì¤Œ
    - np.linspace(start, end, number_of_sample) : samplingì„ ìœ„í•´ ì£¼ë¡œ ì‚¬ìš©ë˜ëŠ” í•¨ìˆ˜ì´ë©°, start~end êµ¬ê°„ ë‚´ì— ìˆëŠ” ë°ì´í„°ë“¤ì„ ì§€ì •í•œ ê°œìˆ˜(number_of_sample) ë§Œí¼ ê· ë“±í•˜ê²Œ ìƒ˜í”Œë§í•˜ì—¬ arrayí˜•íƒœë¡œ ë°˜í™˜í•´ì¤Œ
        - end_point=False ë¡œ ì„¤ì •í•œë‹¤ë©´, ë¦¬ìŠ¤íŠ¸ì˜ ì¸ë±ì‹±ê³¼ ê°™ì´ start ~ (end-1) êµ¬ê°„ìœ¼ë¡œ ì„¤ì •ë˜ë©°, ê¸°ë³¸ ê°’ì€ end_point=True
        - e.g.
            
            ```python
            np.linspace(2.0, 3.0, num=5)
            # -> array([2.  , 2.25, 2.5 , 2.75, 3.  ])
            np.linspace(2.0, 3.0, num=5, endpoint=False)
            # -> array([2. ,  2.2,  2.4,  2.6,  2.8])
            ```
            
    

ìœ„ì˜ ê¸°ë³¸ ì‚¬ìš©ë²•ì„ ë°”íƒ•ìœ¼ë¡œ ê·¸ë˜í”„ë¥¼ ê·¸ë ¤ë³´ì!

```python
foo = lambda x: -(2/7*x**3-9/2*x**2+15*x-10.)

X = np.linspace(0, 10, 100)
y = foo(X)

x_sample = np.linspace(0, 10, 5)
y_sample = foo(x_sample)

plt.plot(X, y)
plt.scatter(x_sample, y_sample, c="red", s=30)
plt.xlabel('x')
plt.ylabel('y')
plt.axhline(0, color='black')
plt.axvline(0, color='black')
plt.show()
```

â†’ ì¶œë ¥ë˜ëŠ” ê·¸ë˜í”„

![plot](img/plot.png)

## 2. Linear Regression

ì„ í˜• íšŒê·€ë€, í•œ ê°œ ì´ìƒì˜ ë…ë¦½ ë³€ìˆ˜ Xì™€ ì¢…ì† ë³€ìˆ˜ y ê°„ì˜ ì„ í˜• ìƒê´€ ê´€ê³„ë¥¼ ëª¨ë¸ë§í•˜ëŠ” íšŒê·€ ë¶„ì„ ê¸°ë²•ì´ë‹¤. (ì¶œì²˜ : [ìœ„í‚¤í”¼ë””ì•„](https://ko.wikipedia.org/wiki/%EC%84%A0%ED%98%95_%ED%9A%8C%EA%B7%80))

[Scikit-learn](https://scikit-learn.org/stable/index.html) ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ì‰½ê²Œ ë§ì€ [ì„ í˜• íšŒê·€](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)ì™€ ê°™ì€ í†µê³„ì ì¸ ëª¨ë¸ + ê¸°ê³„í•™ìŠµ ëª¨ë¸ë“¤ì„ ì •ì˜í•˜ê³  ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.

- Usage
    
    ```python
    from sklearn.linear_model import LinearRegression
    
    # ì„ í˜• íšŒê·€ ëª¨ë¸ ì •ì˜
    lr = LinearRegression()
    
    foo = lambda x: -(2/7*x**3-9/2*x**2+15*x-10.)
    x_sample = np.linspace(0, 10, 5)
    y_sample = foo(x_sample)
    
    # í•˜ë‚˜ì˜ ë°°ì¹˜ ë‹¹ í•˜ë‚˜ì˜ featureë¥¼ ê°€ì§€ë„ë¡ ì°¨ì› ì¶”ê°€
    x_new = x_sample[:, None]
    
    # ì„ í˜• íšŒê·€ ëª¨ë¸ì— fittingí•˜ì—¬ í•™ìŠµ
    lr.fit(x_new, y_sample)
    
    # Coefficient ê³„ì‚°
    r2 = lr.score(x_new, y_sample)
    
    # yê°’ ì˜ˆì¸¡
    y_hat = lr.predict(x_new)
    # ë§Œì•½ í•˜ë‚˜ì˜ ë°ì´í„° í¬ì¸íŠ¸ì— ëŒ€í•œ ì˜ˆì¸¡ê°’ì„ ì–»ê³  ì‹¶ë‹¤ë©´
    # y_hat = lr.predict(x_new[0, None])
    
    # Mean Squared Error ê³„ì‚°
    MSE = np.mean((y_hat - y_sample)**2)
    
    plt.plot(x_new, y_hat)
    ```
    
    â†’ ìœ„ì˜ plotì— ë”í•´ì„œ ê·¸ë¦° ê²½ìš°ì— ëŒ€í•œ ê·¸ë˜í”„
    
    ![plot+lr](img/plot+lr.png)
    

## 3. Polynomial Regression

ë‹¤í•­ íšŒê·€ë€, 2ì°¨ ì´ìƒì˜ ë‹¤í•­ì‹ìœ¼ë¡œ ì´ë£¨ì–´ì§„ ë…ë¦½ ë³€ìˆ˜ Xì™€ ì¢…ì† ë³€ìˆ˜ y ê°„ì˜ ìƒê´€ ê´€ê³„ë¥¼ ëª¨ë¸ë§í•˜ëŠ” íšŒê·€ ë¶„ì„ ê¸°ë²•ì´ë‹¤. (ì¶œì²˜ : [ìœ„í‚¤í”¼ë””ì•„](https://en.wikipedia.org/wiki/Polynomial_regression))

ìœ„ì˜ ì„ í˜• íšŒê·€ ì„¤ëª…ì—ì„œì™€ ë™ì¼í•˜ê²Œ [Scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html)ìœ¼ë¡œ êµ¬í˜„ë˜ì–´ ìˆì–´ ì‰½ê²Œ ì‚¬ìš© ê°€ëŠ¥í•˜ë‹¤.

- Usage

```python
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures

x_sample = np.linspace(0, 10, 5)
x_new = x_sample[:, None]

foo = lambda x: -(2/7*x**3-9/2*x**2+15*x-10.)
y_sample = foo(x_sample)

# 6ì°¨ ë‹¤í•­ì‹ìœ¼ë¡œ ë³€í™˜
poly = PolynomialFeatures(degree=6)
x_sample_poly = poly.fit_transform(x_new)
poly_lr = LinearRegression().fit(x_sample_poly, y_sample)

# -> x_new ì¶œë ¥
# Before transform: (Single features)
# [[ 0. ]
#  [ 2.5]
#  [ 5. ]
#  [ 7.5]
#  [10. ]]

# -> x_sample_poly ì¶œë ¥
# After transform: (Multiple features)
# [[1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
#   0.00000000e+00 0.00000000e+00 0.00000000e+00]
#  [1.00000000e+00 2.50000000e+00 6.25000000e+00 1.56250000e+01
#   3.90625000e+01 9.76562500e+01 2.44140625e+02]
#  [1.00000000e+00 5.00000000e+00 2.50000000e+01 1.25000000e+02
#   6.25000000e+02 3.12500000e+03 1.56250000e+04]
#  [1.00000000e+00 7.50000000e+00 5.62500000e+01 4.21875000e+02
#   3.16406250e+03 2.37304688e+04 1.77978516e+05]
#  [1.00000000e+00 1.00000000e+01 1.00000000e+02 1.00000000e+03
#   1.00000000e+04 1.00000000e+05 1.00000000e+06]]
```

ì•„ë˜ì˜ ê·¸ë¦¼ê³¼ ê°™ì´, í•­ì˜ ì°¨ìˆ˜ê°€ ì»¤ì§ˆ ìˆ˜ë¡ sampleë“¤ì˜ íŠ¹ì§•ì„ ë” ì˜ ë°˜ì˜í•˜ì—¬ ëª¨ë¸ë§ì´ ê°€ëŠ¥í•˜ì§€ë§Œ ë„ˆë¬´ ì»¤ì§€ë©´ Overfitting ë˜ëŠ” ë¬¸ì œê°€ ìˆë‹¤. ë°˜ëŒ€ë¡œ, í•­ì˜ ì°¨ìˆ˜ê°€ ë„ˆë¬´ ë‚®ìœ¼ë©´ Underfitting ë˜ëŠ” ë¬¸ì œê°€ ìˆë‹¤.

![polylr_overfitting_underfitting](img/polylr_overfitting_underfitting.png)

Overfittingì´ë€, training errorëŠ” ì ê³  variance(ì˜ˆì¸¡ê°’ë“¤ì˜ í©ì–´ì§„ ì •ë„)ê°€ í° ë°˜ë©´, test errorê°€ í° í˜„ìƒì´ ë‚˜íƒ€ëŠ” ë¬¸ì œë¥¼ ë§í•œë‹¤. ì¦‰, training dataì— ëŒ€í•´ì„œëŠ” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì¼ ìˆ˜ëŠ” ìˆì–´ë„ í•™ìŠµí•˜ì§€ ì•Šì€ test dataì— ëŒ€í•´ì„œëŠ” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì´ì§€ ëª»í•œë‹¤ëŠ” ë¬¸ì œì ì„ ê°€ì§€ê³  ìˆë‹¤. 

â†’ Overfitting ë¬¸ì œì— ëŒ€í•œ ëŒ€í‘œì ì¸ í•´ê²°ì±…ìœ¼ë¡œëŠ” ë” ë§ì€ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜ Regularization(ì •ê·œí™”)ë¥¼ í•˜ëŠ” ë°©ë²•ì´ ìˆë‹¤.

Underfittingì€ Overfittingê³¼ëŠ” ë°˜ëŒ€ë¡œ training errorì™€ test errorê°€ ë‘˜ ë‹¤ í¬ë©°, bias(ì˜ˆì¸¡ê°’ê³¼ ì •ë‹µê°’ê³¼ì˜ ì°¨ì´) ë˜í•œ í° í˜„ìƒì´ ë‚˜íƒ€ëŠ” ë¬¸ì œë¥¼ ë§í•œë‹¤. ì´ëŠ” ì¶©ë¶„íˆ í•™ìŠµë˜ì§€ ì•Šì•„ ë‚˜íƒ€ëŠ” í˜„ìƒì´ë‹¤. 

â†’ Underfitting ë¬¸ì œì— ëŒ€í•œ í•´ê²°í•˜ê¸° ìœ„í•œ ëŒ€í‘œì ì¸ ë°©ë²•ìœ¼ë¡œëŠ” ë” ë§ì€ featureë¥¼ ì¶”ê°€í•˜ê±°ë‚˜ í•­ì˜ ì°¨ìˆ˜ë¥¼ ì¦ê°€í•˜ì—¬ ëª¨ë¸ì˜ complexityë¥¼ ë†’ì´ê±°ë‚˜ ë” ì˜¤ë˜ í•™ìŠµí•˜ëŠ” ë°©ë²•ì´ í•´ê²°ì±…ì´ ë  ìˆ˜ ìˆë‹¤.

### Regularization (ì •ê·œí™”)

Overfitting ë¬¸ì œë¥¼ ì™„í™”í•˜ê¸° ìœ„í•œ ë°©ë²• ì¤‘ Regularization(ì •ê·œí™”) ë°©ë²•ì€ ëª¨ë¸ì˜ ììœ ë„ë¥¼ ì œí•œí•˜ì—¬ hypothesis spaceë¥¼ ì¤„ì¸ë‹¤. ì¦‰, ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë“¤ì„ ì œí•œì‹œì¼œ varianceë¥¼ ì¤„ì´ê³ , ì¼ë°˜í™”í•˜ëŠ” ëŠ¥ë ¥ì„ ë†’ì¸ë‹¤. ëŒ€í‘œì ì¸ ì •ê·œí™” ë°©ì‹ì—ëŠ” 1) Ridge Regression 2) Lasso Regression 3) Elastic Net ì„¸ ê°€ì§€ê°€ ìˆë‹¤. 

ê° ë°©ì‹ì— ëŒ€í•´ í•˜ë‚˜ì”© ì•Œì•„ë³´ì !

#### **Ridge Regression**

Ridge Regression (ë¦¿ì§€ íšŒê·€) ë°©ë²•ì€ L2 Regressionìœ¼ë¡œ ë¶ˆë¦¬ë©°, í•™ìŠµ ì‹œ Cost function (ë¹„ìš© í•¨ìˆ˜)ì— Lossì˜ ê°€ì¤‘ì¹˜ ê°’ë“¤ì— ëŒ€í•œ L2 regularization termì„ ë”í•˜ì—¬ ëª¨ë¸ì˜ ììœ ë„ë¥¼ ì œí•œí•œë‹¤.

ì˜ˆë¥¼ ë“¤ì–´, 6ì°¨ì›ì˜ ë‹¤í•­ì‹ì— ëŒ€í•´ ë‹¤í•­íšŒê·€ë¥¼ í•  ë•Œ 

<!-- $$w^Tx+b = w_6x^6+w_5x^5+...+w_1x^1+ b$$ --> 

<div align="center"><img style="background: white;" src="https://render.githubusercontent.com/render/math?math="></div>

ë¦¿ì§€ íšŒê·€ ë°©ë²•ì—ì„œì˜ ë¹„ìš© í•¨ìˆ˜ëŠ” ì•„ë˜ì™€ ê°™ì´ í‘œí˜„ëœë‹¤. 

<!-- $$
L(y,y')+\frac{\alpha}{2}\lVert w\rVert^2
$$ --> 

<div align="center"><img style="background: white;" src="https://render.githubusercontent.com/render/math?math=L(y%2Cy')%2B%5Cfrac%7B%5Calpha%7D%7B2%7D%5ClVert%20w%5CrVert%5E2"></div>

ì—¬ê¸°ì—ì„œ <!-- $$\alpha$$ --> 

<div align="center"><img style="background: white;" src="https://render.githubusercontent.com/render/math?math="></div> ëŠ” ì œí•œì˜ ì •ë„(penalty) ì§€ì •í•˜ëŠ” hyperparameterì´ë‹¤. <!-- $$\alpha$$ --> 

<div align="center"><img style="background: white;" src="https://render.githubusercontent.com/render/math?math="></div> ê°€ 0ì´ë©´ linear regression ì´ê³ , <!-- $$\alpha$$ --> 

<div align="center"><img style="background: white;" src="https://render.githubusercontent.com/render/math?math="></div> ê°’ì´ ì»¤ì§ˆ ê²½ìš° ëª¨ë“  ê°€ì¤‘ì¹˜ë“¤ì´ 0ê³¼ ê°€ê¹ê²Œ ë˜ì–´ ì˜ˆì¸¡ ê°’ì´ ë°ì´í„°ë“¤ì˜ í‰ê· ì— ê°€ê¹ê²Œ flat í•œ í˜•íƒœê°€ ëœë‹¤.

![ridge_alpha](img/ridge_alpha.png)

Scikit-learn êµ¬í˜„ì€
 ì•„ë˜ì™€ ê°™ë‹¤.

```python
from sklearn.linear_model import Ridge
ridge_reg = Ridge(alpha=0.1)
ridge_reg.fit(X, y)
ridge_reg.predict([[1.5]])
```

#### **Lasso Regression**

Lasso (**L**east **A**bsolute **S**hrinkage and **S**election **O**perator) Regression ëŠ” ìœ„ì˜ Ridge regression ì—ì„œ ì‚¬ìš©í•œ L2 regularization ëŒ€ì‹ , L1 regularizationì„ í•˜ëŠ” ë°©ë²•ì´ë‹¤.

ìœ„ì— ì„¤ëª…í•œ 6ì°¨ì›ì˜ ë‹¤í•­ì‹ì„ í† ëŒ€ë¡œ, ë¼ì˜ íšŒê·€ì—ì„œì˜ ë¹„ìš© í•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

<!-- $$
L(y,y')+\alpha(|w_6|+|w_5|+...+|w_1|)
$$ --> 

<div align="center"><img style="background: white;" src="https://render.githubusercontent.com/render/math?math=L(y%2Cy')%2B%5Calpha(%7Cw_6%7C%2B%7Cw_5%7C%2B...%2B%7Cw_1%7C)"></div>

ë¼ì˜ íšŒê·€ì˜ í° íŠ¹ì§• ì¤‘ í•˜ë‚˜ëŠ” ì¤‘ìš”í•˜ì§€ ì•Šì€ featureë“¤ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì œê±°í•˜ëŠ” ê²½í–¥ì´ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤. 

- Why? ì—­ì „íŒŒ(Backpropagation) ì‹œì— ë¹„ìš© í•¨ìˆ˜ì— ëŒ€í•´ í¸ë¯¸ë¶„í•˜ê²Œ ë˜ë©´, ê°€ì¤‘ì¹˜ëŠ” ìƒìˆ˜ ê°’ì´ ë˜ì–´ë²„ë¦¬ê¸° ë•Œë¬¸ì—, ê°€ì¤‘ì¹˜ê°€ ë„ˆë¬´ ì‘ì€ ê²½ìš° 0ì— ê°€ê¹Œì›Œ ì¤‘ìš”í•˜ì§€ ì•Šì€ featureë“¤ì˜ ê°€ì¤‘ì¹˜ë“¤ì˜ ì œê±°í•˜ëŠ” íš¨ê³¼ë¥¼ ë‚˜íƒ€ë‚´ê²Œ ëœë‹¤.

ìœ„ì˜ ë¦¿ì§€ íšŒê·€ì˜ ê·¸ë˜í”„ ì´ë¯¸ì§€ì™€ ìœ ì‚¬í•˜ë©´ì„œë„, ê°€ì¤‘ì¹˜ë¥¼ ë” ì‘ê²Œ ì„¤ì •í•˜ë”ë¼ë„ ë” flatí•˜ê²Œ ë§Œë“œëŠ” ê²½í–¥ì´ ë‚˜íƒ€ë‚œë‹¤. ì¦‰, ìë™ì ìœ¼ë¡œ feature selectioní•˜ëŠ” íš¨ê³¼ë¥¼ ë‚˜íƒ€ë‚´ê³  sparse modelì„ ë„ì¶œí•´ë‚¸ë‹¤.

![lasso_alpha](img/lasso_alpha.png)

Scikit-learnìœ¼ë¡œ ì‰½ê²Œ ì‚¬ìš© ê°€ëŠ¥í•˜ë‹¤.

```python
from sklearn.linear_model import Lasso
lasso_reg = Lasso(alpha=0.1)
lasso_reg.fit(X, y)
lasso_reg.predict([[1.5]])
```

#### **Elastic Net** 

Elastic Netì€ Ridge Regression ê³¼ Lasso Regressionì˜ ì¤‘ê°„ìœ¼ë¡œ, ë‘ regularization termì„ ì„ì–´ ì‚¬ìš©í•œë‹¤. mix ratio r ìœ¼ë¡œ ì¡°ì ˆí•  ìˆ˜ ìˆë‹¤.

<!-- $$
L(y,y')+r\alpha(\sum_{i=1}^n|w_i|)+\frac{l-r}{2}\alpha\lVert w\rVert^2
$$ --> 

<div align="center"><img style="background: white;" src="https://render.githubusercontent.com/render/math?math=L(y%2Cy')%2Br%5Calpha(%5Csum_%7Bi%3D1%7D%5En%7Cw_i%7C)%2B%5Cfrac%7Bl-r%7D%7B2%7D%5Calpha%5ClVert%20w%5CrVert%5E2"></div>

ì„¸ ê°€ì§€ ì •ê·œí™” ë°©ë²• ì¤‘ì— ë³´í†µ Ridgeë¥¼ ê¸°ë³¸ìœ¼ë¡œ ë§ì´ ì‚¬ìš©ë˜ì§€ë§Œ, ì¼ë¶€ ì ì€ featureë§Œ ìœ ìš©í•  ë•Œ Lasso ë‚˜ Elastic Netì„ ì‚¬ìš©í•œë‹¤. ë³´í†µ Lassoì˜ ê²½ìš° featureì˜ ê°œìˆ˜ê°€ training instanceë³´ë‹¤ ë§ê±°ë‚˜ ì¼ë¶€ featureë“¤ì´ correlateë  ë•Œ ì˜ˆì¸¡ì´ ì–´ë ¤ìš°ë¯€ë¡œ, Lasso ë³´ë‹¤ Elastic Netì´ ì„ í˜¸ëœë‹¤.

Scikit-learnìœ¼ë¡œ ì‰½ê²Œ ì‚¬ìš© ê°€ëŠ¥í•˜ë‹¤.

```python
from sklearn.linear_model import ElasticNet
elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)
elastic_net.fit(X, y)
elastic_net.predict([[1.5]])
```

### ë‹¤í•­ íšŒê·€ vs ë¦¿ì§€ íšŒê·€

ë‹¤í•­ íšŒê·€ì— ëŒ€í•´ ë¦¿ì§€ ì •ê·œí™”ì˜ ì ìš©í•œ ë¦¿ì§€ íšŒê·€ ë˜í•œ [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ì‰½ê²Œ ì ìš© ê°€ëŠ¥í•˜ë‹¤.

- Implementation
    
    ```python
    from sklearn.linear_model import LinearRegression
    from sklearn.preprocessing import PolynomialFeatures
    from sklearn.linear_model import Ridge
    
    x_line = np.linspace(0, 10, 100)
    x_sample = np.linspace(0, 10, 5)
    x_new = x_sample[:, None]
    
    foo = lambda x: -(2/7*x**3-9/2*x**2+15*x-10.)
    Y = foo(x_line)
    y_sample = foo(x_sample)
    # 6ì°¨ ë‹¤í•­ì‹ìœ¼ë¡œ ë³€í™˜
    poly = PolynomialFeatures(degree=6)
    x_sample_poly = poly.fit_transform(x_new)
    x_line_poly = poly.fit_transform(x_line[:, None])
    
    # ë‹¤í•­ íšŒê·€ ì ìš© ë° ì˜ˆì¸¡
    poly_lr = LinearRegression().fit(x_sample_poly, y_sample)
    y_poly = poly_lr.predict(x_line_poly)
    
    # Ridge íšŒê·€ ì ìš© ë° ì˜ˆì¸¡
    # penaltyì˜ ì •ë„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” lambdaëŠ” 'alpha'ë¼ëŠ” hyperparameterë¡œ ì‚¬ìš©ë¨ !
    rr = Ridge(alpha=10.0).fit(x_sample_poly, y_sample)
    y_poly_rr = rr.predict(x_line_poly)
    ```
    
    â†’ Plotìœ¼ë¡œ í‘œí˜„
    
    (blue: ì •ë‹µ ê°’, orange: ë‹¤í•­ íšŒê·€ ì˜ˆì¸¡ ê°’, green: ë¦¿ì§€ íšŒê·€ ê°’)
    
    ![plot_poly_ridge](img/plot_poly_ridge.png)
    

## 4. Classification (Logistic Regression, Support Vector Machine, Decision Tree)

ëŒ€í‘œì ì¸ Classification(ë¶„ë¥˜) ëª¨ë¸ìœ¼ë¡œëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì„¸ ê°€ì§€ê°€ ìˆë‹¤.

- Logistic Regression (ë¡œì§€ìŠ¤í‹± íšŒê·€)
- Support Vector Machine (SVM) (ì„œí¬íŠ¸ ë²¡í„° ë¨¸ì‹ )
- Decision Tree (ê²°ì • íŠ¸ë¦¬)

### Logistic Regression (ë¡œì§€ìŠ¤í‹± íšŒê·€)

ë¡œì§€ìŠ¤í‹± íšŒê·€ëŠ” ì„ í˜• íšŒê·€ì™€ ê°™ì´ ì…ë ¥ featureë“¤ì˜ weighted sumì„ ê³„ì‚°í•˜ì§€ë§Œ, ê²°ê³¼ë¥¼ ë°”ë¡œ ì¶œë ¥í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ê²°ê³¼ì˜ logisticì„ ì¶œë ¥í•œë‹¤. logistic ì€ sigmoid functionì„ ëœ»í•˜ë©° 0ê³¼ 1 ì‚¬ì´ì˜ ê°’ì„ ê°€ì§„ë‹¤. 

![sigmoid](img/sigmoid.png)

- logit (log-odds) : logistic functionì˜ ì—­ìœ¼ë¡œ, positive classì¸ì§€ negative classì¸ì§€ ì¸¡ì •ëœ í™•ë¥ (p)ì˜ ratioì˜ log ê°’ì´ë‹¤.
    
    <!-- $$logit(p) = log(p/(1-p))$$ --> 

<div align="center"><img style="background: white;" src="https://render.githubusercontent.com/render/math?math="></div>
    

closed-form equationì´ ì•„ë‹ˆê¸° ë•Œë¬¸ì—, ë¹„ìš©í•¨ìˆ˜ë¥¼ ìµœì†Œí™” í•˜ëŠ” parameterë¥¼ ê³„ì‚°í•˜ëŠ” ê²ƒì´ ì–´ë µì§€ë§Œ, ë¹„ìš©í•¨ìˆ˜ê°€ convex í˜•íƒœì¼ ê²½ìš° Gradient Descentì™€ ê°™ì€ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•œë‹¤ë©´ global optimumì„ ì°¾ì„ ìˆ˜ ìˆë‹¤.

- multiple classesì˜ ê²½ìš°, softmax regression(multinomial logistic regression)ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. ì´ ë•Œ, í´ë˜ìŠ¤ì™€ í´ë˜ìŠ¤ ë³„ í™•ë¥ ì„ matchí•˜ì—¬ ì¸¡ì •í•˜ëŠ” cross entropy ë¹„ìš© í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œë‹¤.

### Support Vector Machine (SVM) (ì„œí¬íŠ¸ ë²¡í„° ë¨¸ì‹ )

ì„œí¬íŠ¸ ë²¡í„° ë¨¸ì‹  ë¶„ë¥˜ëŠ” ì‰½ê²Œ ì„¤ëª…í•˜ìë©´, ë¶„ë¥˜í•˜ê³ ì í•˜ëŠ” ë‘ í´ë˜ìŠ¤ ê°„ì˜ ê±°ë¦¬(margin)ë¥¼ ìµœëŒ€í™”í•˜ëŠ” ëª¨ë¸ì´ë‹¤. small or medium í¬ê¸°ì˜ ë°ì´í„° ì…‹ì— ì í•©í•œ ëª¨ë¸ì´ë‹¤

![svm](img/svm.png)

ë‘ ì¹´í…Œê³ ë¦¬ ì¤‘ ì–´ëŠ í•˜ë‚˜ì— ì†í•œ ë°ì´í„°ì˜ ì§‘í•©ì´ ì£¼ì–´ì¡Œì„ë•Œ, ì£¼ì–´ì§„ ë°ì´í„° ì§‘í•©ì„ ë°”íƒ•ìœ¼ë¡œ ìƒˆë¡œìš´ ë°ì´í„°ê°€ ì–´ëŠ ì¹´í…Œê³ ë¦¬ì— ì†í• ì§€ íŒë‹¨í•˜ëŠ” ë¹„í™•ë¥ ì  ì´ì§„ ì„ í˜• ë¶„ë¥˜ ëª¨ë¸ì„ ë§Œë“ ë‹¤. ì´ ë•Œ, ë°ì´í„°ê°€ ì„ë² ë”©ëœ ê³µê°„ì—ì„œ ê²½ê³„(boundary)ë¥¼ í‘œí˜„í•  ë•Œ ê°€ì¥ í° í­(large margin)ì„ ê°€ì§„ ê²½ê³„ë¥¼ ì°¾ëŠ”ë‹¤. ì¦‰, ê°€ì¥ ê°€ê¹Œìš´ ê° í´ë˜ìŠ¤ì˜ ë°ì´í„° ì ë“¤ ê°„ì˜ ê±°ë¦¬ë¥¼ ìµœëŒ€ë¡œ í•œë‹¤.

ë•Œë¬¸ì—, í•™ìŠµì´ ì§„í–‰ë˜ëŠ” ë™ì•ˆ SVMì€ ê° í›ˆë ¨ ë°ì´í„° í¬ì¸íŠ¸ê°€ ë‘ í´ë˜ìŠ¤ ì‚¬ì´ì˜ ê²°ì • ê²½ê³„ë¥¼ êµ¬ë¶„í•˜ëŠ” ë° ì–¼ë§ˆë‚˜ ì¤‘ìš”í•œ ì§€ë¥¼ ë°°ìš°ê²Œ ëœë‹¤. ë°ì´í„°ì…‹ ì „ì²´ê°€ ì•„ë‹Œ í´ë˜ìŠ¤ ì‚¬ì´ì˜ ê²½ê³„ì— ìœ„ì¹˜í•œ ë°ì´í„° í¬ì¸íŠ¸ë“¤ì´ ê²°ì • ê²½ê³„ë¥¼ ë§Œë“œëŠ” ë° ì˜í–¥ì„ ì¤€ë‹¤. ì´ëŸ¬í•œ ë°ì´í„° í¬ì¸íŠ¸ë“¤ì„ ì„œí¬íŠ¸ ë²¡í„°(support vector)ë¼ê³  í•œë‹¤.

ì„ í˜• ë¶„ë¥˜ ë¿ë§Œ ì•„ë‹ˆë¼ ë¹„ì„ í˜• ë¶„ë¥˜ì—ì„œë„ ì‚¬ìš©ë  ìˆ˜ ìˆìœ¼ë©°, ì„ í˜•ì ìœ¼ë¡œ ë¶„ë¥˜ê°€ ì–´ë ¤ìš´ ë°ì´í„°ì— Featureë¥¼ ë”í•´(polynomial feature) ê³ ì°¨ì› ê³µê°„ìœ¼ë¡œ ëŒ€ì‘ì‹œì¼œ ë¶„ë¦¬ë¥¼ ì‰½ê²Œ í•˜ëŠ” ë°©ë²•ì„ ì‚¬ìš©í•œë‹¤. polynomial degreeê°€ í´ ìˆ˜ë¡ ëª¨ë¸ì´ ëŠë ¤ì§€ê¸° ë•Œë¬¸ì—, ë¬¸ì œì— ì ì ˆí•œ kernel trickì„ ì‚¬ìš©í•œë‹¤. ì´ëŠ” ì ë“¤ì˜ ì§‘í•©ê³¼ ìƒìˆ˜ ë²¡í„°ì˜ ë‚´ì  ì—°ì‚°ìœ¼ë¡œ ì •ì˜í•˜ì—¬ íš¨ìœ¨ì ìœ¼ë¡œ ê³„ì‚°í•˜ë„ë¡ ë•ëŠ”ë‹¤. í° ë°ì´í„°ì…‹ì˜ ê²½ìš°, Gaussian RBF Kernelì„ ì‚¬ìš©í•œë‹¤.

![svm_kernel](img/svm_kernel.png)

ì¥ì  : ë¶„ë¥˜, ì˜ˆì¸¡ì— ì‚¬ìš© ê°€ëŠ¥. overfitting ì •ë„ê°€ ëœí•˜ë‹¤. ì˜ˆì¸¡ì˜ ì •í™•ë„ê°€ ë†’ê³ , ì‚¬ìš©í•˜ê¸° ì‰¬ì›€

ë‹¨ì  : kernel, parameter ì¡°ì ˆ í…ŒìŠ¤íŠ¸ë¥¼ ì—¬ëŸ¬ë²ˆ í•´ì•¼ ìµœì í™”ëœ ëª¨ë¸ì„ ë§Œë“¤ ìˆ˜ ìˆìŒ, ëª¨ë¸ êµ¬ì¶• ì‹œê°„ ì˜¤ë˜ê±¸ë¦¼

### Decision Tree (ê²°ì • íŠ¸ë¦¬)

ê²°ì • íŠ¸ë¦¬ ì•Œê³ ë¦¬ì¦˜ì€ Featureì— ëŒ€í•´ ì•„ë˜ì˜ ê·¸ë¦¼ê³¼ ê°™ì´ íŠ¸ë¦¬ ìë£Œ êµ¬ì¡° ê¸°ë°˜ìœ¼ë¡œ ë¶„ë¥˜í•œë‹¤. 

![dt](img/dt.png)

ê²°ì • íŠ¸ë¦¬ëŠ” feature scalingì´ë‚˜ centeringê³¼ ê°™ì€ ë°ì´í„° ì „ì²˜ë¦¬ê°€ í•„ìš”í•˜ì§€ ì•Šë‹¤.

Scikit-Learn ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œì˜ êµ¬í˜„ì€ Classification and Regression Tree(CART) ì•Œê³ ë¦¬ì¦˜ì„ ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµí•œë‹¤.

ë¨¼ì €, í•™ìŠµ ë°ì´í„°ì— ëŒ€í•´ í•˜ë‚˜ì˜ feature <!-- $$k$$ --> 

<div align="center"><img style="background: white;" src="https://render.githubusercontent.com/render/math?math="></div> ì™€ ê·¸ì— ëŒ€í•œ threshold <!-- $$t_k$$ --> 

<div align="center"><img style="background: white;" src="https://render.githubusercontent.com/render/math?math="></div> ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‘ ê°œì˜ subsetìœ¼ë¡œ ë‚˜ëˆˆë‹¤. ì´ ë•Œ, thresholdëŠ” ì˜ ë¶„ë¦¬ëœ(Purest) Subsetì´ ë˜ë„ë¡ <!-- $$(k, t_k)$$ --> 

<div align="center"><img style="background: white;" src="https://render.githubusercontent.com/render/math?math="></div> ìŒì„ ì°¾ì•„ì„œ ì„¤ì •í•œë‹¤.

<!-- $$
J(k,t_k)=\frac{m_{left}}{m}G_{left}+\frac{m_{right}}{m}G_{right}
$$ --> 

<div align="center"><img style="background: white;" src="https://render.githubusercontent.com/render/math?math=J(k%2Ct_k)%3D%5Cfrac%7Bm_%7Bleft%7D%7D%7Bm%7DG_%7Bleft%7D%2B%5Cfrac%7Bm_%7Bright%7D%7D%7Bm%7DG_%7Bright%7D"></div>

<!-- $$G_{left/right}$$ --> 

<div align="center"><img style="background: white;" src="https://render.githubusercontent.com/render/math?math="></div> ëŠ” left, right subsetì˜ ì˜ ë¶„ë¦¬ë˜ì§€ ì•ŠìŒì˜ ì •ë„(impurity)ë¥¼ ëœ»í•˜ê³ , <!-- $$m_{left/right}$$ --> 

<div align="center"><img style="background: white;" src="https://render.githubusercontent.com/render/math?math="></div> ì€ left, right ê° subsetì˜ ê°œìˆ˜ë¥¼ ëœ»í•œë‹¤. 

Regression ë¬¸ì œì— ì ìš©í•œë‹¤ë©´, <!-- $$G_{left/right}$$ --> 

<div align="center"><img style="background: white;" src="https://render.githubusercontent.com/render/math?math="></div> ëŒ€ì‹  <!-- $$MSE_{left/right}$$ --> 

<div align="center"><img style="background: white;" src="https://render.githubusercontent.com/render/math?math="></div> lossë¥¼ ì‚¬ìš©í•œë‹¤.

<!-- $$
MSE_{node} = \sum_{i \in node}(\hat y_{node} - y^{(i)})^2 \\ \hat y_{node}= \frac{1}{m_{node}}\sum_{i \in node}y^{(i)}
$$ --> 

<div align="center"><img style="background: white;" src="https://render.githubusercontent.com/render/math?math=MSE_%7Bnode%7D%20%3D%20%5Csum_%7Bi%20%5Cin%20node%7D(%5Chat%20y_%7Bnode%7D%20-%20y%5E%7B(i)%7D)%5E2%20%5C%5C%20%5Chat%20y_%7Bnode%7D%3D%20%5Cfrac%7B1%7D%7Bm_%7Bnode%7D%7D%5Csum_%7Bi%20%5Cin%20node%7Dy%5E%7B(i)%7D"></div>

**max_depth** parameterë¡œ ë‘ì–´, ìœ„ì˜ ê³¼ì •ì„ ì¬ê·€ì ìœ¼ë¡œ ë°˜ë³µí•˜ë©° depthë§Œí¼ì˜ íŠ¸ë¦¬ë¥¼ êµ¬ì¶•í•œë‹¤.

ì„ í˜• ëª¨ë¸ê³¼ ë‹¬ë¦¬ ê²°ì • íŠ¸ë¦¬ ëª¨ë¸ì˜ ê²½ìš° í•™ìŠµ ë°ì´í„°ì— ëŒ€í•˜ ì œí•œí•˜ëŠ” ì •ë„ê°€ ì ê¸° ë•Œë¬¸ì—, í•™ìŠµ ë°ì´í„°ì— íŠ¸ë¦¬ êµ¬ì¡°ê°€ ë„ˆë¬´ adaptí•˜ê²Œ í•™ìŠµ ë  ê²½ìš°ì— overfitting ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆë‹¤. íŒŒë¼ë¯¸í„°ê°€ ì—†ë‹¤ê¸° ë³´ë‹¤ íŒŒë¼ë¯¸í„°ë¡œ ì¸í•œ ì œí•œì´ í•™ìŠµ ì´ì „ì— ì—†ëŠ” ëª¨ë¸ì„ nonparametric ëª¨ë¸ì´ë¼ê³  ë¶€ë¥¸ë‹¤. ë°˜ëŒ€ë¡œ parametric ëª¨ë¸ì€ degree of freedomì´ ì œí•œë˜ê¸° ë•Œë¬¸ì— overfittingì˜ ìœ„í—˜ì„ ì¤„ì¼ ìˆ˜ ìˆë‹¤. overfittingì„ ì¤„ì´ê¸° ìœ„í•´ì„œ freedomì„ ì œí•œí•˜ê¸° ìœ„í•´ ì •ê·œí™”(regularization)ì„ ì ìš©í•œë‹¤. ìì„¸í•œ hyperparameterëŠ” [ì—¬ê¸°](https://scikit-learn.org/stable/modules/tree.html#tips-on-practical-use)ì— ì„¤ëª…ë˜ì–´ ìˆë‹¤.

### Implementation

ìœ„ì˜ ì„¸ ëª¨ë¸ ([Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html), [Support Vector Machine](https://scikit-learn.org/stable/modules/svm.html#), [Decision Tree](https://scikit-learn.org/stable/modules/tree.html#)) ëª¨ë‘ Scikit-learn ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ê°„ë‹¨í•˜ê²Œ êµ¬í˜„í•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.

```python
from sklearn.linear_model import LogisticRegression

logistic = LogisticRegression(random_state=1234)
logistic.fit(X_train[:, :2], y_train)
# softmax regression
# softmax_reg = LogisticRegression(multi_class="multinomial")

from sklearn.svm import SVC

svm = SVC(kernel='linear', random_state=1234)
svm.fit(X_train[:, :2], y_train)

from sklearn.tree import DecisionTreeClassifier
# more depth, increase decision boundary - leads to overfitting 
# similar to polynomial
dt = DecisionTreeClassifier(max_depth=2, random_state=1234) 
dt.fit(X_train[:, :2], y_train)
```

ğŸ‘‰ğŸ¼ ê´€ë ¨ ì‹¤ìŠµ ì½”ë“œ :

[https://github.com/Sunkyoung/PyTorch-Study/blob/main/PyTorch_Study_02_Basic_ML.ipynb](https://github.com/Sunkyoung/PyTorch-Study/blob/main/PyTorch_Study_02_Basic_ML.ipynb)

**Reference**

Aurelien Geron, Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow, Oâ€™reilly (2019)

```toc

```