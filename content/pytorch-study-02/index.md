## 1. Matplotlib

NumPy ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í™œìš©í•˜ì—¬ ê·¸ë˜í”„ë¥¼ ë§Œë“¤ì–´ ì‹œê°í™”í•  ìˆ˜ ìˆëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ì´ë‹¤.

ì§€ë‚œë²ˆ NumPyì— ëŒ€í•œ ê¸°ë³¸ì ì¸ ì„¤ëª…ì€ ì—¬ê¸°ì—! ğŸ‘‡ğŸ¼

[https://sunkyoung.github.io/pytorch-study-01/](https://sunkyoung.github.io/pytorch-study-01/)

- How to use ?

```python
import matplotlib.pyplot as plt
import numpy as np
```

- Basic usage
    - plt.plot(x-axis, y-axis) : ì£¼ì–´ì§„ xì¶•, yì¶• ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì„ í˜• ê·¸ë˜í”„ë¥¼ ê·¸ë¦¼
    - plt.scatter(x-axis, y-axis, s=None, c=None) : ì£¼ì–´ì§„ xì¶•, yì¶• ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì ì„ ê·¸ë¦¼
        - s : ì ì˜ í¬ê¸°
        - c : ìƒ‰ê¹” ì§€ì • (ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œë„ ì§€ì •ê°€ëŠ¥í•˜ë©° ë¦¬ìŠ¤íŠ¸ ê¸¸ì´ë§Œí¼ cmapê³¼ normì„ mappingí•˜ì—¬ ìƒ‰ì„ í‘œí˜„)
        - e.g. `plt.scatter(X, y, s=30, c="red")`
    - plt.show() : í”Œë¡¯ì„ ë³´ì—¬ì¤Œ
    - np.linspace(start, end, number_of_sample) : samplingì„ ìœ„í•´ ì£¼ë¡œ ì‚¬ìš©ë˜ëŠ” í•¨ìˆ˜ì´ë©°, start~end êµ¬ê°„ ë‚´ì— ìˆëŠ” ë°ì´í„°ë“¤ì„ ì§€ì •í•œ ê°œìˆ˜(number_of_sample) ë§Œí¼ ê· ë“±í•˜ê²Œ ìƒ˜í”Œë§í•˜ì—¬ arrayí˜•íƒœë¡œ ë°˜í™˜í•´ì¤Œ
        - end_point=False ë¡œ ì„¤ì •í•œë‹¤ë©´, ë¦¬ìŠ¤íŠ¸ì˜ ì¸ë±ì‹±ê³¼ ê°™ì´ start ~ (end-1) êµ¬ê°„ìœ¼ë¡œ ì„¤ì •ë˜ë©°, ê¸°ë³¸ ê°’ì€ end_point=True
        - e.g.
          
            ```python
            np.linspace(2.0, 3.0, num=5)
            # -> array([2.  , 2.25, 2.5 , 2.75, 3.  ])
            np.linspace(2.0, 3.0, num=5, endpoint=False)
            # -> array([2. ,  2.2,  2.4,  2.6,  2.8])
            ```
            
    

ìœ„ì˜ ê¸°ë³¸ ì‚¬ìš©ë²•ì„ ë°”íƒ•ìœ¼ë¡œ ê·¸ë˜í”„ë¥¼ ê·¸ë ¤ë³´ì!

```python
foo = lambda x: -(2/7*x**3-9/2*x**2+15*x-10.)

X = np.linspace(0, 10, 100)
y = foo(X)

x_sample = np.linspace(0, 10, 5)
y_sample = foo(x_sample)

plt.plot(X, y)
plt.scatter(x_sample, y_sample, c="red", s=30)
plt.xlabel('x')
plt.ylabel('y')
plt.axhline(0, color='black')
plt.axvline(0, color='black')
plt.show()
```

â†’ ì¶œë ¥ë˜ëŠ” ê·¸ë˜í”„

![plot](plot.png)

## 2. Linear Regression

ì„ í˜• íšŒê·€ë€, í•œ ê°œ ì´ìƒì˜ ë…ë¦½ ë³€ìˆ˜ Xì™€ ì¢…ì† ë³€ìˆ˜ y ê°„ì˜ ì„ í˜• ìƒê´€ ê´€ê³„ë¥¼ ëª¨ë¸ë§í•˜ëŠ” íšŒê·€ ë¶„ì„ ê¸°ë²•ì´ë‹¤. (ì¶œì²˜ : [ìœ„í‚¤í”¼ë””ì•„](https://ko.wikipedia.org/wiki/%EC%84%A0%ED%98%95_%ED%9A%8C%EA%B7%80))

[Scikit-learn](https://scikit-learn.org/stable/index.html) ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ì‰½ê²Œ ë§ì€ [ì„ í˜• íšŒê·€](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)ì™€ ê°™ì€ í†µê³„ì ì¸ ëª¨ë¸ + ê¸°ê³„í•™ìŠµ ëª¨ë¸ë“¤ì„ ì •ì˜í•˜ê³  ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.

- Usage
  
    ```python
    from sklearn.linear_model import LinearRegression
    # ì„ í˜• íšŒê·€ ëª¨ë¸ ì •ì˜
    lr = LinearRegression()
    
    foo = lambda x: -(2/7*x**3-9/2*x**2+15*x-10.)
    x_sample = np.linspace(0, 10, 5)
    y_sample = foo(x_sample)
    
    # í•˜ë‚˜ì˜ ë°°ì¹˜ ë‹¹ í•˜ë‚˜ì˜ featureë¥¼ ê°€ì§€ë„ë¡ ì°¨ì› ì¶”ê°€
    x_new = x_sample[:, None]
    
    # ì„ í˜• íšŒê·€ ëª¨ë¸ì— fittingí•˜ì—¬ í•™ìŠµ
    lr.fit(x_new, y_sample)
    
    # Coefficient ê³„ì‚°
    r2 = lr.score(x_new, y_sample)
    
    # yê°’ ì˜ˆì¸¡
    y_hat = lr.predict(x_new)
    # ë§Œì•½ í•˜ë‚˜ì˜ ë°ì´í„° í¬ì¸íŠ¸ì— ëŒ€í•œ ì˜ˆì¸¡ê°’ì„ ì–»ê³  ì‹¶ë‹¤ë©´
    # y_hat = lr.predict(x_new[0, None])
    
    # Mean Squared Error ê³„ì‚°
    MSE = np.mean((y_hat - y_sample)**2)
    
    plt.plot(x_new, y_hat)
    ```
    
    â†’ ìœ„ì˜ plotì— ë”í•´ì„œ ê·¸ë¦° ê²½ìš°ì— ëŒ€í•œ ê·¸ë˜í”„
    
    ![plot+lr](plot+lr.png)
    

## 3. Polynomial Regression

ë‹¤í•­ íšŒê·€ë€, 2ì°¨ ì´ìƒì˜ ë‹¤í•­ì‹ìœ¼ë¡œ ì´ë£¨ì–´ì§„ ë…ë¦½ ë³€ìˆ˜ Xì™€ ì¢…ì† ë³€ìˆ˜ y ê°„ì˜  ìƒê´€ ê´€ê³„ë¥¼ ëª¨ë¸ë§í•˜ëŠ” íšŒê·€ ë¶„ì„ ê¸°ë²•ì´ë‹¤. (ì¶œì²˜ : [ìœ„í‚¤í”¼ë””ì•„](https://en.wikipedia.org/wiki/Polynomial_regression))



## 4. Classification (Logistic Regression, Support Vector Machine, Decision Tree)



ğŸ‘‰ğŸ¼ ê´€ë ¨ ì‹¤ìŠµ ì½”ë“œ :

[https://github.com/Sunkyoung/PyTorch-Study/blob/main/PyTorch_Study_02_Basic_ML.ipynb](https://github.com/Sunkyoung/PyTorch-Study/blob/main/PyTorch_Study_02_Basic_ML.ipynb)


```toc

```